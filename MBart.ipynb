{"cells":[{"cell_type":"markdown","metadata":{"id":"c3SlS3-qGqFV"},"source":["##### Authors: Nay Lin\n","This notebook wants to test MBart before and after adding the self-consistency layer."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20164,"status":"ok","timestamp":1711524477820,"user":{"displayName":"CS4248 Team 10","userId":"08926647107743523271"},"user_tz":-480},"id":"qPJvQGtGHL7_","outputId":"bed561fb-b564-4e8f-c668-88a3073fac64"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6311,"status":"ok","timestamp":1711524484128,"user":{"displayName":"CS4248 Team 10","userId":"08926647107743523271"},"user_tz":-480},"id":"Ckz9sTX-GqFW","outputId":"cdedb683-17cc-44e6-f0f0-f3c22c26f7e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["0      Last year I showed these two slides so that  d...\n","1      But this understates the seriousness of this p...\n","2      The arctic ice cap is, in a sense,  the beatin...\n","3          It expands in winter and contracts in summer.\n","4      The next slide I show you will be  a rapid fas...\n","                             ...                        \n","874    You increase paralysis, and you decrease satis...\n","875                          Everybody needs a fishbowl.\n","876    This one is almost certainly too limited --  p...\n","877    But the absence of some metaphorical fishbowl ...\n","878                                 Thank you very much.\n","Name: en, Length: 879, dtype: object\n","0      去年我给各位展示了两个 关于北极冰帽的演示 在过去三百万年中 其面积由相当于美国南方48州面...\n","1                        但这些没能完全说明这个问题的严重性 因为这没有表示出冰帽的厚度\n","2                               感觉上，北极冰帽 就好象全球气候系统中跳动的心脏\n","3                                          冬天心脏舒张，夏天心脏收缩\n","4                                  下面我要展示的是 在过去25年里的极剧变化\n","                             ...                        \n","874                                    你增加了的是瘫痪，减少了的是满足。\n","875                                       每个人都需要这么个“鱼缸”。\n","876                  至于这个（鱼缸）嘛，对这条鱼 可能是小了点，对于我们几乎肯定是太小了。\n","877                    但是，没有这么个象征性的鱼缸那就 意味着苦难将至， 也许是，灾难。\n","878                                                非常感谢。\n","Name: zh, Length: 879, dtype: object\n"]}],"source":["import pandas as pd\n","\n","# Load the data\n","train_df = pd.read_csv('/content/drive/MyDrive/data/train.csv')\n","test_df = pd.read_csv('/content/drive/MyDrive/data/test.csv')\n","validation_df = pd.read_csv('/content/drive/MyDrive/data/validation.csv')\n","\n","# Sanity check\n","print(validation_df['en'])\n","print(validation_df['zh'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eiNWBVIoGqFX"},"outputs":[],"source":["from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n","import torch\n","\n","model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n","model = MBartForConditionalGeneration.from_pretrained(model_name)\n","tokenizer = MBart50TokenizerFast.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1070,"status":"ok","timestamp":1711527755887,"user":{"displayName":"CS4248 Team 10","userId":"08926647107743523271"},"user_tz":-480},"id":"6-e_8fMkGqFY","outputId":"26f41516-2bd6-418c-ba6a-90779d815162"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","English: Hello, how are you?\n","Chinese: 你好,你好吗?\n","Back-translated: Hello, how are you?\n"]}],"source":["# Translation Test\n","src_lang = \"en_XX\"\n","tgt_lang = \"zh_CN\"\n","\n","# Func. def. if using GPU\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(device)\n","model = model.to(device)\n","\n","def translate(model, tokenizer, sentence, src_lang, tgt_lang):\n","    tokenizer.src_lang = src_lang\n","    tokenizer.tgt_lang = tgt_lang\n","    encoded_input = tokenizer(sentence, return_tensors=\"pt\").to(device)\n","    generated_tokens = model.generate(**encoded_input, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang])\n","    translation = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n","    return translation\n","\n","# To only use CPU, run the previous cell + this func. def. below\n","# def translate(model, tokenizer, sentence, tgt_lang):\n","#     encoded_input = tokenizer(sentence, return_tensors=\"pt\")\n","#     generated_tokens = model.generate(**encoded_input, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang])\n","#     translation = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n","#     return translation\n","\n","english_sentence = \"Hello, how are you?\"\n","chinese_translation = translate(model, tokenizer, english_sentence, src_lang, tgt_lang)\n","back_translate = translate(model, tokenizer, chinese_translation, tgt_lang, src_lang)\n","print(f\"English: {english_sentence}\\nChinese: {chinese_translation}\\nBack-translated: {back_translate}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6498,"status":"ok","timestamp":1711524839054,"user":{"displayName":"CS4248 Team 10","userId":"08926647107743523271"},"user_tz":-480},"id":"AhS42E6qJ__L","outputId":"da26331c-dcf6-408b-915a-1405c2a59cad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sacrebleu\n","  Downloading sacrebleu-2.4.1-py3-none-any.whl (106 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting portalocker (from sacrebleu)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.25.2)\n","Collecting colorama (from sacrebleu)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n","Installing collected packages: portalocker, colorama, sacrebleu\n","Successfully installed colorama-0.4.6 portalocker-2.8.2 sacrebleu-2.4.1\n"]}],"source":["!pip3 install sacrebleu"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":511,"status":"ok","timestamp":1711295449635,"user":{"displayName":"CS4248 Team 10","userId":"08926647107743523271"},"user_tz":-480},"id":"Da5ihxr1GqFY","outputId":"f223c182-fdcf-4479-cfe3-62d6552088e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["BLEU Score: 28.317815064640417\n"]}],"source":["# NOTE: If you already ran this before, load the translations from a text file and get the BLEU Score\n","with open('/content/drive/MyDrive/mbarttranslations.txt', 'r', encoding='utf-8') as f:\n","    hyp = [line.strip() for line in f.readlines()]\n","refs = [[row['zh']] for _, row in validation_df.iterrows()]\n","import sacrebleu\n","bleu_score = sacrebleu.corpus_bleu(hyp, refs, tokenize='zh')\n","print(f\"BLEU Score: {bleu_score.score}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dtZhgSzrGqFY"},"outputs":[],"source":["# Method to perform SacreBleu test on pretrained model (en to zh)\n","import sacrebleu\n","def translate_and_evaluate(df, model, tokenizer, src_lang, tgt_lang):\n","    if src_lang == \"en\":\n","      tokenizer.src_lang, tokenizer.tgt_lang = \"en_XX\", \"zh_CN\"\n","    else:\n","      tokenizer.src_lang, tokenizer.tgt_lang = \"zh_CN\", \"en_XX\"\n","\n","    hyp = []\n","    refs = [[] for _ in range(len(df))]\n","    for idx, row in df.iterrows():\n","        # Translate the sentence\n","        encoded_input = tokenizer(row[src_lang], return_tensors=\"pt\").to(device)\n","        generated_tokens = model.generate(**encoded_input, forced_bos_token_id=tokenizer.lang_code_to_id[tokenizer.tgt_lang])\n","        translation = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n","        hyp.append(translation)\n","\n","        # Prepare references\n","        refs[idx].append(row[tgt_lang])\n","\n","    # Compute BLEU score\n","    if tgt_lang == \"en\":\n","      sacrebleu_without_smoothing = sacrebleu.corpus_bleu(hyp, refs)\n","    else:\n","      sacrebleu_without_smoothing = sacrebleu.corpus_bleu(hyp, refs, tokenize='zh')\n","    return hyp, sacrebleu_without_smoothing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q7cmlEHvGqFY","outputId":"2f0d0ed6-21b2-4b3d-8906-7bc0a4a997c6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711520473898,"user_tz":-480,"elapsed":579405,"user":{"displayName":"CS4248 Team 10","userId":"08926647107743523271"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["BLEU Score: 28.317815064640417\n"]}],"source":["# Perform the forward translation and evaluation\n","translations, bleu_score = translate_and_evaluate(validation_df, model, tokenizer, 'en', 'zh')\n","print(f\"BLEU Score: {bleu_score.score}\")\n","\n","# Save translations to a file\n","with open('forwardmbarttranslations.txt', 'w', encoding='utf-8') as f:\n","    for translation in translations:\n","        f.write(translation + '\\n')"]},{"cell_type":"code","source":["# Perform the backward translation and evaluation\n","translations, bleu_score = translate_and_evaluate(validation_df, model, tokenizer, 'zh', 'en')\n","print(f\"BLEU Score: {bleu_score.score}\")\n","# Save translations to a file\n","with open('backwardmbarttranslations.txt', 'w', encoding='utf-8') as f:\n","    for translation in translations:\n","        f.write(translation + '\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W2FTAms3ib0p","executionInfo":{"status":"ok","timestamp":1711525423458,"user_tz":-480,"elapsed":568394,"user":{"displayName":"CS4248 Team 10","userId":"08926647107743523271"}},"outputId":"0b4f1aed-9312-40ce-a906-2f60dda4c6ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BLEU Score: 52.511563937151365\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoH-LycgGqFZ"},"outputs":[],"source":["# Define self-consistency simply using cosine_similarity\n","# TODO: Test other metrics e.g. MSE, etc.\n","import torch\n","from torch.nn.functional import cosine_similarity\n","\n","def self_consistency_loss(original_text, model, tokenizer):\n","    # Translate to Chinese\n","    translated_text = translate(model, tokenizer, original_text, \"zh_CN\")\n","\n","    # Translate back to English\n","    back_translated_text = translate(model, tokenizer, translated_text, \"en_XX\")\n","\n","    # Encoded texts\n","    encoded_input = tokenizer(original_text, return_tensors=\"pt\").to(device)\n","    encoded_back_translated_input = tokenizer(back_translated_text, return_tensors=\"pt\").to(device)\n","\n","    # Calculate cosine similarity between original and back-translated text embeddings\n","    original_embedding = model.get_encoder()(**encoded_input).last_hidden_state.mean(dim=1)\n","    back_translated_embedding = model.get_encoder()(**encoded_back_translated_input).last_hidden_state.mean(dim=1)\n","    loss = 1 - cosine_similarity(original_embedding, back_translated_embedding)\n","\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NLThZJ-dGqFZ","outputId":"ec43d4e1-f957-4697-f574-555df37070cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average English Token Length: 25.429808099763907\n","Average Chinese Token Length: 22.599063416152827\n"]}],"source":["# Checking the maximum length of the tokens\n","en_token_len = [len(tokenizer.encode(text)) for text in train_df['en']]\n","avg_en_token_len = sum(en_token_len) / len(en_token_len)\n","print(f\"Average English Token Length: {avg_en_token_len}\")\n","\n","zh_token_len = [len(tokenizer.encode(text)) for text in train_df['zh']]\n","avg_zh_token_len = sum(zh_token_len) / len(zh_token_len)\n","print(f\"Average Chinese Token Length: {avg_zh_token_len}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qpvg6jblGqFZ"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","# You can adjust\n","# 1. max_length\n","# 2. batch_size\n","\n","class TranslationDataset(Dataset):\n","    # Here I chose max length 32 because it is bigger than 25.4 and 22.6\n","    def __init__(self, dataframe, tokenizer, max_length=32):\n","        self.dataframe = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        english_text = self.dataframe.iloc[idx]['en']\n","        chinese_text = self.dataframe.iloc[idx]['zh']\n","        tokenized_input = self.tokenizer(english_text, return_tensors=\"pt\", max_length=self.max_length, padding=\"max_length\", truncation=True)\n","        tokenized_target = self.tokenizer(chinese_text, return_tensors=\"pt\", max_length=self.max_length, padding=\"max_length\", truncation=True)\n","        input_ids = tokenized_input.input_ids.squeeze()\n","        attention_mask = tokenized_input.attention_mask.squeeze()\n","        labels = tokenized_target.input_ids.squeeze()\n","        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n","\n","# Create the dataset\n","dataset = TranslationDataset(train_df, tokenizer)\n","\n","# Create the DataLoader\n","dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"UOGgjAmDGqFZ","outputId":"297e6190-814e-4949-ee52-0ab658628303"},"outputs":[{"output_type":"stream","name":"stdout","text":["Restarting from 0 and batch 401\n","Total number of batches: 14455\n","Checkpoint saved at epoch 0, batch 500\n","Batch number 500: Epoch 0, Batch Loss: 1.1399003267288208, Estimated Time Left: 64508.99039309978\n","Checkpoint saved at epoch 0, batch 600\n","Batch number 600: Epoch 0, Batch Loss: 1.7609258890151978, Estimated Time Left: 106936.80470516084\n","Checkpoint saved at epoch 0, batch 700\n","Batch number 700: Epoch 0, Batch Loss: 1.4337968826293945, Estimated Time Left: 135701.80808919668\n","Checkpoint saved at epoch 0, batch 800\n","Batch number 800: Epoch 0, Batch Loss: 1.4458568096160889, Estimated Time Left: 156712.67868054807\n","Checkpoint saved at epoch 0, batch 900\n","Batch number 900: Epoch 0, Batch Loss: 1.5686205625534058, Estimated Time Left: 172539.733075281\n","Checkpoint saved at epoch 0, batch 1000\n","Batch number 1000: Epoch 0, Batch Loss: 1.2339277267456055, Estimated Time Left: 185966.03737805845\n","Checkpoint saved at epoch 0, batch 1100\n","Batch number 1100: Epoch 0, Batch Loss: 1.3911930322647095, Estimated Time Left: 196523.19516154853\n","Checkpoint saved at epoch 0, batch 1200\n","Batch number 1200: Epoch 0, Batch Loss: 1.3462181091308594, Estimated Time Left: 205333.88932642143\n","Checkpoint saved at epoch 0, batch 1300\n","Batch number 1300: Epoch 0, Batch Loss: 1.109614610671997, Estimated Time Left: 211782.67312062427\n","Checkpoint saved at epoch 0, batch 1400\n","Batch number 1400: Epoch 0, Batch Loss: 1.4719922542572021, Estimated Time Left: 217342.85115849975\n","Checkpoint saved at epoch 0, batch 1500\n","Batch number 1500: Epoch 0, Batch Loss: 2.0008108615875244, Estimated Time Left: 221844.8034438348\n","Checkpoint saved at epoch 0, batch 1600\n","Batch number 1600: Epoch 0, Batch Loss: 1.5149544477462769, Estimated Time Left: 225458.36904585065\n","Checkpoint saved at epoch 0, batch 1700\n","Batch number 1700: Epoch 0, Batch Loss: 1.4139275550842285, Estimated Time Left: 228118.9490251387\n","Checkpoint saved at epoch 0, batch 1800\n","Batch number 1800: Epoch 0, Batch Loss: 1.6752572059631348, Estimated Time Left: 230405.03350216016\n","Checkpoint saved at epoch 0, batch 1900\n","Batch number 1900: Epoch 0, Batch Loss: 1.6602727174758911, Estimated Time Left: 232560.214232574\n","Checkpoint saved at epoch 0, batch 2000\n","Batch number 2000: Epoch 0, Batch Loss: 1.6036722660064697, Estimated Time Left: 234100.3302838224\n","Checkpoint saved at epoch 0, batch 2100\n","Batch number 2100: Epoch 0, Batch Loss: 1.665537714958191, Estimated Time Left: 235250.9638935367\n","Checkpoint saved at epoch 0, batch 2200\n","Batch number 2200: Epoch 0, Batch Loss: 1.8461449146270752, Estimated Time Left: 235881.31927944854\n","Checkpoint saved at epoch 0, batch 2300\n","Batch number 2300: Epoch 0, Batch Loss: 1.62626314163208, Estimated Time Left: 236262.17589869036\n","Checkpoint saved at epoch 0, batch 2400\n","Batch number 2400: Epoch 0, Batch Loss: 1.1861324310302734, Estimated Time Left: 236391.23094568652\n","Checkpoint saved at epoch 0, batch 2500\n","Batch number 2500: Epoch 0, Batch Loss: 1.4161951541900635, Estimated Time Left: 236537.9372763791\n","Checkpoint saved at epoch 0, batch 2600\n","Batch number 2600: Epoch 0, Batch Loss: 1.2423611879348755, Estimated Time Left: 236663.65769935845\n","Checkpoint saved at epoch 0, batch 2700\n","Batch number 2700: Epoch 0, Batch Loss: 1.4932124614715576, Estimated Time Left: 236498.6206850842\n","Checkpoint saved at epoch 0, batch 2800\n","Batch number 2800: Epoch 0, Batch Loss: 1.3278323411941528, Estimated Time Left: 236060.5574866891\n","Checkpoint saved at epoch 0, batch 2900\n","Batch number 2900: Epoch 0, Batch Loss: 1.4150102138519287, Estimated Time Left: 235537.43482219148\n","Checkpoint saved at epoch 0, batch 3000\n","Batch number 3000: Epoch 0, Batch Loss: 1.3647748231887817, Estimated Time Left: 234929.08025205173\n","Checkpoint saved at epoch 0, batch 3100\n","Batch number 3100: Epoch 0, Batch Loss: 1.226449728012085, Estimated Time Left: 234151.5317185498\n","Checkpoint saved at epoch 0, batch 3200\n","Batch number 3200: Epoch 0, Batch Loss: 1.1536765098571777, Estimated Time Left: 233550.6661207337\n","Checkpoint saved at epoch 0, batch 3300\n","Batch number 3300: Epoch 0, Batch Loss: 1.507841944694519, Estimated Time Left: 232603.66307013071\n","Checkpoint saved at epoch 0, batch 3400\n","Batch number 3400: Epoch 0, Batch Loss: 1.006062626838684, Estimated Time Left: 231696.7745460023\n","Checkpoint saved at epoch 0, batch 3500\n","Batch number 3500: Epoch 0, Batch Loss: 0.9977331757545471, Estimated Time Left: 230600.2046310258\n","Checkpoint saved at epoch 0, batch 3600\n","Batch number 3600: Epoch 0, Batch Loss: 1.2996604442596436, Estimated Time Left: 229368.4113584065\n","Checkpoint saved at epoch 0, batch 3700\n","Batch number 3700: Epoch 0, Batch Loss: 1.8937803506851196, Estimated Time Left: 228192.5630330221\n","Checkpoint saved at epoch 0, batch 3800\n","Batch number 3800: Epoch 0, Batch Loss: 1.5879549980163574, Estimated Time Left: 226927.78975796106\n","Checkpoint saved at epoch 0, batch 3900\n","Batch number 3900: Epoch 0, Batch Loss: 1.4764033555984497, Estimated Time Left: 225653.12363674334\n","Checkpoint saved at epoch 0, batch 4000\n","Batch number 4000: Epoch 0, Batch Loss: 1.127333402633667, Estimated Time Left: 224376.28743138313\n"]}],"source":["import time\n","import os\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","\n","def save_checkpoint(model, optimizer, epoch, batch_index, checkpoint_path):\n","    torch.save({\n","        'epoch': epoch,\n","        'batch_index': batch_index,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict()\n","    }, checkpoint_path)\n","    print(f\"Checkpoint saved at epoch {epoch}, batch {batch_index}\")\n","\n","# Define the checkpoint path\n","checkpoint_dir = \"/content/drive/MyDrive/checkpoints\"\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","# Check if a checkpoint exists\n","latest_checkpoint_path = os.path.join(checkpoint_dir, \"latest_checkpoint.pt\")\n","if os.path.exists(latest_checkpoint_path):\n","    # Load the checkpoint\n","    checkpoint = torch.load(latest_checkpoint_path, map_location='cpu')\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    start_epoch = checkpoint['epoch']\n","    start_batch_index = checkpoint['batch_index'] + 1\n","    print(f\"Restarting from {start_epoch} and batch {start_batch_index}\")\n","else:\n","    start_epoch = 0\n","    start_batch_index = 0\n","\n","# Hyper-Parameters\n","num_epochs = 1\n","consistency_weight = 0.1 # Weight to self-consistency loss vs translation loss\n","num_batches = len(dataloader)\n","print(f\"Total number of batches: {num_batches}\")\n","\n","# Training Loop\n","model.train()\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    start_time = time.time()\n","    for batch_index, batch in enumerate(dataloader, start=1):\n","        if batch_index < start_batch_index:\n","            continue\n","\n","        # Reset the gradients after each mini batch\n","        optimizer.zero_grad()\n","\n","        # Move data to GPU\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","        # input_ids = batch[\"input_ids\"]\n","        # attention_mask = batch[\"attention_mask\"]\n","        # labels = batch[\"labels\"]\n","\n","        # Forward pass\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        translation_loss = outputs.loss\n","\n","        # Self-consistency loss\n","        consistency_losses = []\n","        for input_id in input_ids:\n","            text = tokenizer.decode(input_id, skip_special_tokens=True)\n","            loss = self_consistency_loss(text, model, tokenizer).to(device)\n","            # loss = self_consistency_loss(text, model, tokenizer)\n","            consistency_losses.append(loss)\n","        consistency_loss = torch.mean(torch.stack(consistency_losses))\n","\n","        # Total loss\n","        total_loss = translation_loss + consistency_weight * consistency_loss\n","\n","        # Backward pass and optimization\n","        total_loss.backward()\n","        optimizer.step()\n","\n","        if batch_index % 100 == 0:\n","            # checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}_batch_{batch_index}.pt\")\n","            # save_checkpoint(model, optimizer, epoch, batch_index, checkpoint_path)\n","            # Update the latest checkpoint path\n","            save_checkpoint(model, optimizer, epoch, batch_index, latest_checkpoint_path)\n","\n","            # Time:\n","            curr_time = time.time()\n","            remaining_duration = ((curr_time - start_time) / (batch_index)) * (num_batches - batch_index)\n","            print(f\"Batch number {batch_index}: Epoch {epoch}, Batch Loss: {total_loss.item()}, Estimated Time Left: {remaining_duration}\")\n","\n","    # Reset the start_batch_index after completing an epoch\n","    start_batch_index = 0\n","\n","    print(f\"Epoch {epoch + 1} completed\")\n"]},{"cell_type":"code","source":["# Load the checkpoint\n","latest_checkpoint_path = \"/content/drive/MyDrive/data/mbart_latest_4000_of_14455.pt\"\n","checkpoint = torch.load(latest_checkpoint_path, map_location='cpu')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","start_epoch = checkpoint['epoch']\n","start_batch_index = checkpoint['batch_index']\n","print(f\"Loaded from epoch {start_epoch} and batch {start_batch_index}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mZCfjFkQfgtM","executionInfo":{"status":"ok","timestamp":1711527790989,"user_tz":-480,"elapsed":10887,"user":{"displayName":"CS4248 Team 10","userId":"08926647107743523271"}},"outputId":"d940358d-1f11-4a02-dc90-7d4d1630acd2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded from epoch 0 and batch 4000\n"]}]},{"cell_type":"code","source":["# Testing forward translation\n","# Perform the translation and evaluation\n","translations, bleu_score = translate_and_evaluate(validation_df, model, tokenizer, 'en', 'zh')\n","print(f\"BLEU Score: {bleu_score.score}\")\n","\n","# Save translations to a file\n","with open('tunedforwardmbarttranslations.txt', 'w', encoding='utf-8') as f:\n","    for translation in translations:\n","        f.write(translation + '\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V03U0LJ1twH0","executionInfo":{"status":"ok","timestamp":1711524294786,"user_tz":-480,"elapsed":538297,"user":{"displayName":"CS4248 Team 10","userId":"08926647107743523271"}},"outputId":"8a8b2e66-ee30-421f-b3f5-06fdfc8e1fd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BLEU Score: 30.682659738699808\n"]}]},{"cell_type":"code","source":["def translate(model, tokenizer, sentence, src_lang, tgt_lang):\n","    tokenizer.src_lang = src_lang\n","    tokenizer.tgt_lang = tgt_lang\n","    encoded_input = tokenizer(sentence, return_tensors=\"pt\").to(device)\n","    generated_tokens = model.generate(**encoded_input, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang])\n","    translation = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n","    return translation\n","\n","print(translate(model, tokenizer, \"你好吗?\", 'zh_CN', 'en_XX'))\n","print(translate(model, tokenizer, \"Can you please work?\", 'en_XX', 'zh_CN'))\n","print(translate(model, tokenizer, \"आज मेरा जन्मदिन हे\", 'hi_IN', 'en_XX'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fyuvs-Og7Elh","executionInfo":{"status":"ok","timestamp":1711527802681,"user_tz":-480,"elapsed":1289,"user":{"displayName":"CS4248 Team 10","userId":"08926647107743523271"}},"outputId":"ec198a0b-85ed-403e-d5a9-21f40087ad3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["你好吗?\n","你能工作吗?\n","今天是我生日。\n"]}]},{"cell_type":"code","source":["# Testing backward translation\n","# Perform the translation and evaluation\n","translations, bleu_score = translate_and_evaluate(validation_df, model, tokenizer, 'zh', 'en')\n","print(f\"BLEU Score: {bleu_score.score}\")\n","\n","# Save translations to a file\n","with open('tunedbackwardmbarttranslations.txt', 'w', encoding='utf-8') as f:\n","    for translation in translations:\n","        f.write(translation + '\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSFOAg1KyRYj","executionInfo":{"status":"ok","timestamp":1711526023993,"user_tz":-480,"elapsed":464084,"user":{"displayName":"CS4248 Team 10","userId":"08926647107743523271"}},"outputId":"87708a3d-a61d-4583-c83f-8dceaafa2e6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BLEU Score: 0.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsbzbzzLGqFa"},"outputs":[],"source":["# Save the Model\n","torch.save(model.state_dict(), \"mbart_translation_model_with_self_consistency.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vACdqlaRGqFa"},"outputs":[],"source":["# Load the Model\n","model_path = \"mbart_translation_model_with_self_consistency.pth\"\n","model.load_state_dict(torch.load(model_path))\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Lec-Dc6GqFa"},"outputs":[],"source":["# TODO: Test the Model\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}